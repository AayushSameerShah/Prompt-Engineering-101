# 1ï¸âƒ£ Ensembling

## `1.` Mixture of Reasoning Experts (MoRE)

ğŸ”— Link [Mixture of Reasoning Experts (MoRE)](https://learnprompting.org/docs/advanced/ensembling/mixture_of_reasoning_experts_more)

<img src="./images/more_cover.svg" alt="Mixture of Reasoning Experts (MoRE)" width=400>

> ***This technique uses different LLMs (differently prompted LLMs) which are expert in different things, and then combines their answers to get a better answer.***

### There are generaly differnt types, but to understand:

1. Factual reasoning (e.g., fact-based questions).
2. Multihop reasoning (e.g., questions that require multiple steps of reasoning).
3. Mathematical reasoning (e.g., solving math word problems).
4. Commonsense reasoning (e.g., questions requiring implicit knowledge).

> MoRE uses an **answer selector** to choose the best response based on predictions from the specialized experts.

> If the system detects that none of the answers are reliable, it can abstain from answering. Another key feature of MoRE is its ability to abstain from answering when it's unsure, improving the system's reliability.

### ğŸ–¼ï¸ Visualizing the MoRE Architecture

1. **Expert Specialization & Reasoning** <br>
   <img src="./images/more_1.svg" alt="Mixture of Reasoning Experts (MoRE) - Step 1" width=400>

---

2. **Answer Selection & Final Output** <br>
   <img src="./images/more_2.svg" alt="Mixture of Reasoning Experts (MoRE) - Step 2" width=400>
