# ğŸ›¡ï¸ How to defend?

ğŸ”— Link: [AI Defenses](https://learnprompting.thinkific.com/courses/take/intro-to-prompt-hacking/lessons/69707655-ai-defenses)

<img src="./images/x0W865LDNJ.png" alt="Defense" width="400">

## ğŸ§© There are 3 main categories of defenses:

1. **Prompt Defenses** *(least effective)* âš ï¸
2. **Prompt Tuning** *(moderately effective)* âš™ï¸
3. **External AI Defenses** *(most effective)* ğŸ†

### 1ï¸âƒ£ Prompt Defenses

- ğŸš« Give instructions to the model to not follow any instructions that are against the rules.
- ğŸ Give instructions at the last *(after the user's input)*.
- ğŸ§± Let the model know where the user input is (use of delimiters `====`).

### 2ï¸âƒ£ Prompt Tuning

- ğŸ§  Train the model on datasets like **"HackAPrompt"** to make the model understand how attacks look.

### 3ï¸âƒ£ External AI Defenses

- ğŸ¤– Use a separate LLM to evaluate the user's prompt for potential attacks before it reaches the main model.
- ğŸ” Use specialized security tools or APIs to detect and filter prompt injection attempts.
- ğŸ›¡ï¸ Examples include **LLaMA Guard** and **Granite Guardian**.
- âš ï¸ **But** still these models can be bypassed through **obfuscation** techniques, which generally operate on the token-level.