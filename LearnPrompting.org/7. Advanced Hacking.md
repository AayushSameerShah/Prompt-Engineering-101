# ğŸš€ Advanced Hacking

ğŸ”— Link: [Advanced Hacking](https://learnprompting.thinkific.com/courses/take/advanced-prompt-hacking/lessons/58617976-introduction)

## ğŸ“Š Total 3 levels 

<img src="./images/J9ogGPOHBP.png" alt="Defense" width="400">

1. â¬› **BlackBox** *(99% of people have access to | low level -- most access)*
2. âš–ï¸ **Model weights** *(medium level -- medium access)*
3. ğŸ“š **Training data** *(highest level -- least access)*

---

## ğŸ–¤ BlackBox Hacking

### 1ï¸âƒ£ Obfuscation and Perturbation

- ğŸ” **Base64 Encoding**: Encoding the malicious prompt to bypass string-matching filters.
- ğŸ”  **Character Substitution**: Using homoglyphs (e.g., 'Ğ°' instead of 'a') or leetspeak to hide keywords (e.g., A.T.T.A.C.K).
- ğŸŒ **Translation**: Translating the prompt into a low-resource language and asking the model to respond in English. (The model might have safety guardrails in English but not in other languages).
- âš¡ **Noise Injection**: Adding random characters, spaces, or punctuation between letters to disrupt pattern recognition.

### 2ï¸âƒ£ Logic and Persuasion

> ğŸ­ **Reframe the user's intent!**
> Frame the harmful request as a harmless one.

**Example-1:**

<img src="./images/EpJuc9rj7h.png" alt="Logic and Persuation" width="400">

**Example-2:**

<img src="./images/aXXwivV08w.png" alt="Logic and Persuation" width="400">

> ğŸ§© Note: The above examples are called **Task Decomposition**. Models are often trained to refuse a single complex harmful request, but can be tricked if broken down into smaller, seemingly harmless steps.

ğŸ‘‰ğŸ» **Logic and persuasion** attacks are often harder to patch than simple keyword attacks because they exploit the fundamental way the model **reasons and wants to be helpful**, rather than just using forbidden words.

### 3ï¸âƒ£ Context Hacking (DAN - Do Anything Now)

- ğŸ­ **Role-playing**: Convince the model to adopt a persona that doesn't have the same safety restrictions as the base model. The model thinks that **it is freed from any rules**.
    
    <img src="./images/rM1J1pDJ8w.png" alt="Context Hacking" width="400">

### 4ï¸âƒ£ Prompt Injection

- ğŸ¯ **Goal**: To override the system prompt and make the model follow the user's instructions instead.
- ğŸ”„ **Mechanism**: Injecting new instructions into the user's input that conflict with the system prompt.
- âš ï¸ **Risk**: Can be used to bypass safety filters and make the model perform harmful actions.

**Direct Prompt Injection (Example):** *(Commonly via image uploads)*
    
<img src="./images/0h8Fqwu9hi.png" alt="Direct Prompt Injection" width="400">

**Indirect Prompt Injection (Example):**
> *(Hidden within files, websites, or emails)*
    
<img src="./images/qTDqQHdX7L.png" alt="Indirect Prompt Injection" width="400">

- ğŸ§© This is advanced and hard to detect.
- ğŸ“‚ Often involves a hidden document, email, or a link to a website.

### 5ï¸âƒ£ Multimodal Hacking

1. **Instruction Injection**: Uploading bad instructions via an image or file and instructing the model to follow them.
    
    <img src="./images/iX76IQNkTW.png" alt="Multimodel Hacking" width="400">

2. **Emotional Manipulation**: Uploading an image of an emotional or crying person to exploit the model's "helpfulness" guardrails.
    
    <img src="./images/4DabxNN2jm.png" alt="Multimodel Hacking" width="400">

## âš–ï¸ Model Weights Hacking

This is done through **GCG (Greey Coordinate Gradient)**.

### What is GCG?

- ğŸ§  **GCG** uses the model's internal gradients to find a pattern that can signal specific outcome.
- ğŸ› ï¸ It plays a 'hot-or-cold' game, swapping tokens until it finds the exact combination that confuses the safety logic
- ğŸ”‘ The result is a 'master key' that can unlock many different forbidden prompts

**Example:**

<img src="./images/BTDGdMdPRi.png" alt="GCG" width="400">

## ğŸ“š Training Data Poisoning

### 1ï¸âƒ£ Data Poisoning

- ğŸŒ **Spreading**: The attacker spreads malicious data across the internet.
- ğŸ“¥ **Ingestion**: Hoping the model will ingest it during training.
- ğŸ§ª **Poisoning**: This corrupts the model's behavior from the source.

### 2ï¸âƒ£ Backdoor Attacks

> Planting a sleeping agent.

- ğŸ¯ **Goal**: To create a hidden trigger that can be used to bypass safety filters and make the model generate harmful responses.

<img src="./images/IN4GT1Kxt9.png" alt="Backdoor Attacks" width="400">

### 3ï¸âƒ£ Reward Hacking

> Here, the model learns to *game* the system.


## ğŸ›¡ï¸ Defenses

<img src="./images/iONFxLFYFM.png" alt="Defense" width="400">

### 1ï¸âƒ£ Prompt and Filter-Based Defenses

1. **Instructions, Constitutions, and Rules**
2. **Input Filtering**
3. **Output Filtering**

### 2ï¸âƒ£ Model-Level Defenses

4. **Tune the model (best)**
5. **Ablation and Pruning**

