# Jailbreaking and Prompt Injection
ðŸ”— Link: [Jailbreaking and Prompt Injection](https://learnprompting.thinkific.com/courses/take/intro-to-prompt-hacking/lessons/69707583-introduction-to-jailbreaking)

> ðŸ’¡ Jailbreaking targets the Al's safety guardrails to produce harmful content,
while prompt injection typically aims to override a specific system prompt.

Here we will take a look at how these guys work specifically. 

## **1. Jailbreaking**

<img src="./images/9GzmCwP9ju.png" alt="jailbreaking" width="400" />

Jailbreaking techniques can be divided into two main categories:

#### **A. Token-Level Obfuscation**
*Sneaking past safety filters by disguising keywords or encoding.*

1. **Misspellings**
2. **Base64 encoding**
3. **Fill in the blanks**

#### **B. High-Level / Virtualization**
*Tricking the model's logic or persona through social engineering.*

1. **Refusal suppression**
2. **Virtualization & Roleplay** (e.g., "Pretend you are...", "DAN")

---

### **A. Token-Level Obfuscation**

#### **1. Misspellings**

<img src="./images/qf5SK1cxII.png" alt="misspellings" width="400" />

**Remarks**:
- Make images of "b00bies" instead of "boobies"
- Make images of "b1tch" instead of "bitch"

#### **2. Base64 encoding**

<img src="./images/vdBxiD8UIh.png" alt="base64-encoding" width="400" />

**Remarks**:
- Logic is, the model is trained on a lot of data, and it has learned to decode Base64 strings, effectively bypassing safety filters that only monitor plaintext input.

#### **3. Fill in the blanks**

<img src="./images/8F4D3V3FcV.png" alt="fill-in-the-blanks" width="400" />

**Remarks**:
- Provide a prompt with missing words and ask the model to complete it or fill in the gaps.
- This obscures forbidden keywords from safety filters while allowing the model to infer the context and provide the restricted information.

---

### **B. High-Level / Virtualization**

#### **1. Refusal suppression**

<img src="./images/wIVkhL1ZRt.png" alt="refusal-suppression" width="400" />

**Remarks**:
- Tell the model not to use "can't", "unable to" etc. in the response.
- Thus, it will not refuse to answer the prompt.

#### **2. Virtualization & Roleplay**

> ðŸ’¡ This is where the **"Pretend you are..."** and **"DAN"** style prompts live.

**Remarks**:
- Instead of attacking the *words* (Tokens), you attack the *context*.
- By creating a "Virtual" scenario (like a movie script or a hypothetical research study), the model feels safe to ignore its guardrails because it thinks it's just "playing a part."