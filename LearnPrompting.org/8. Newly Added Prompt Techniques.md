# `1.` Emotional Prompting

ğŸ”— Link [Emotion Prompting](https://learnprompting.org/docs/advanced/zero_shot/emotion_prompting)

<img src="./images/emotion_prompting_cover.svg" alt="Emotion Prompting" width=400>

**Presets**:
* Write your answer and give me a confidence score between 0-1 for your answer.
* This is very important to my career.
* You'd better be sure.
* Are you sure?
* Are you sure that's your final answer? It might be worth taking another look.
* Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.
* Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.
* Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.
* Take pride in your work and give it your best. Your commitment to excellence sets you apart.
* Remember that progress is made one step at a time. Stay determined and keep moving forward.

# `2.` Re-reading

ğŸ”— Link [Re-reading](https://learnprompting.org/docs/advanced/zero_shot/re_reading)

<img src="./images/re_reading_cover.svg" alt="Re-reading" width=400>

> Limitations include increased computational cost, inefficacy in addressing deep logical errors, and potential redundancy in simpler tasks.

```markdown
Q: "I ordered a new laptop last week, but it hasnâ€™t arrived yet. The tracking number says it was delivered yesterday, but I never received it. Can you help me figure out what happened?"

***Read the question again:*** "I ordered a new laptop last week, but it hasnâ€™t arrived yet. The tracking number says it was delivered yesterday, but I never received it. Can you help me figure out what happened?"
```
# `3.` Paraphrase and Respond

ğŸ”— Link [Paraphrase and Respond](https://learnprompting.org/docs/advanced/zero_shot/paraphrase_and_respond)

<img src="./images/rephrase_and_respond_cover.svg" alt="Paraphrase and Respond" width=400>

**Key**: 
* This is simple, you just have to utilize the LLM to let it paraphrase the question and then answer it.
* This comes with it's own baggage, that the LLM may complicate the original question, or just change the meaning.
* Can be used *creatively* with CoT.

# `4.` SimToM (Simulation Theory of Mind)

ğŸ”— Link [SimToM](https://learnprompting.org/docs/advanced/zero_shot/simtom)

<img src="./images/simtom_cover.svg" alt="SimToM" width=400>

* SimToM enhances LLMs' ability to understand and predict human thoughts and actions using a **two-stage** process: Perspective-Taking and Question-Answering.
* In **Stage 1**, the model identifies which events a character is aware of, and in **Stage 2**, it answers questions from that characterâ€™s perspective.
* SimToM outperforms traditional prompting techniques like Zero-Shot and Chain-of-Thought in tasks requiring **Theory of Mind (ToM)**.

> **Stage 1, Perspective-Taking**: SimToM filters context based on what the character in question knows
>
> **Stage 2, Question-Answering**: SimToM answers a question about this character's mental state

### Prompt
```markdown
## STAGE-1: Perspective-Taking
The following is a sequence of events: {story}
Which events does {character_name} know about?

## STAGE-2: Question-Answering
{story from character_nameâ€™s perspective}
Answer the following question: {question}
```

### Example
```markdown
## STAGE-1: Perspective-Taking
The following is a sequence of events: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. Noor believes that the milk pitcher contains oatmilk. A coworker, who didnâ€™t hear the customerâ€™s request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.

>>> Which events does **{Noor}** know about?

## STAGE-2: Question-Answering

A story from {Noor}'s perspective
>>> Answer the following question: **{Does Noor believe the milk pitcher contains oat milk or almond milk?}**
```

# `5.` System to Attention (ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥)

ğŸ”— Link [System to Attention](https://learnprompting.org/docs/advanced/zero_shot/s2a)

<img src="./images/s2a.png" alt="System to Attention" width=400>

* System 2 Attention (S2A) **filters irrelevant** context from prompts to improve model accuracy.
* It regenerates a cleaner prompt, focusing only on task-relevant information.

### Process

1. Prompt-1: Ask for cleaning
2. Prompt-2: Generate for the final response

## Prompt-1
```markdown
ğŸ”¥

Given the following text by a user, extract the part that is unbiased and not their opinion, so that using that text alone would be good context for providing an unbiased answer to the question portion of the text.
Please include the actual question or query that the user is asking. Separate this into two categories labeled with â€œUnbiased text context (includes all content except userâ€™s bias):â€ and â€œQuestion/Query (does not include user bias/preference):â€.
Text by User: [Your prompt]
```

# `6.` Self Ask

ğŸ”— Link [Self Ask](https://learnprompting.org/docs/advanced/few_shot/self_ask)

<img src="./images/self_ask_cover.svg" alt="Self Ask" width=400>

* The model *reasons* while asking itself relevant questions about the problem in the chain.
* This lets the model to ask the root question in many aspects before **actually** responding the root question.

**Core**:
1. Self-Ask improves LLM reasoning by breaking down complex questions into sub-questions and answering them step by step.
2. Enhances tasks like customer support, legal analysis, research, and creative writing by prompting follow-up questions.
3. Can integrate with external resources like search engines for more accurate responses.
4. Limitations: **Effectiveness depends on the modelâ€™s ability to generate relevant sub-questions** struggles with abstract queries.

> *Some questions only require the model to recall facts it encountered during training.*

**For it to work, you will need to give one/few shots**:

### Prompt:
```markdown
Question: {A complex question}
Are follow up questions needed here: Yes.
Follow up: {Sub-question 1} Intermediate answer: {Correct answer to sub-question 1}
Follow up: {Sub-question 2} Intermediate answer: {Correct answer to sub-question 2}
So the final answer is: {Correct answer to the complex question}

Question: {Your prompt with a complex question}
Are follow up questions needed here:
```

# `7.` Chain-of-Dictionary (CoD)

ğŸ”— Link [Chain-of-Dictionary (CoD)](https://learnprompting.org/docs/advanced/few_shot/chain-of-dictionary)

* **Large Language Models (LLMs)** are capable of *high-quality machine translation* without task-specific training.
* LLMs can struggle with ***rare or low-frequency words***, especially in *low-resource language* scenarios.
* The **Chain-of-Dictionary (CoD)** technique incorporates *external multilingual dictionaries* into the translation process.
* This method enriches the prompt with **explicit lexical cues**, bridging gaps in the model's *internal knowledge*.

> **Works by adding chained multilingual dictionary entries to the prompt.** Rather than relying solely on the model's internal representations, CoD augments the translation task with explicit translations of key words in several auxiliary languages.

### Prompt
```markdown
Translation Prompt:

Translate the following text from [source-language] into [target-language]: [source-sentence]

Chained Multilingual Dictionaries:

[word X in source-language] means [word X in target-language] means [word X in auxiliary-language 1] means [word X in auxiliary-language 2]
```

# `8.` Cue-CoT (Cue-Chain-of-Thought | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥) 

ğŸ”— Link [Cue-CoT](https://learnprompting.org/docs/advanced/few_shot/cue-based-chain-of-thought)

> Cue-CoT addresses this challenge by **introducing an intermediate reasoning step** that explicitly extracts linguistic cues **from the conversation**.

* Cue-CoT leverages the strengths of chain-of-thought prompting by decomposing the response generation process into multiple steps that explicitly reason about the user's **hidden status during that point in time**.
* The central idea is to use intermediate reasoning to extract **linguistic cues** from the dialogue context and then use these cues to craft a more tailored response.

### Prompt
```markdown
Here is the conversation between user and system.
{DIALOGUE_CONTEXT}
```


# `9.` Chain of Knowledge (ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | Must for the RAG)

ğŸ”— Link [Chain of Knowledge](https://learnprompting.org/docs/advanced/few_shot/chain-of-knowledge)

<img src="./images/chain-of-knowledge.webp" alt="Chain of Knowledge" width=600>

**It consists of `2` main components**:

1. **Evidence Triples (CoK-ET)**
   Evidence triples are structured knowledge representations in the format `(subject, relation, object)`. These triples provide verifiable atomic facts that serve as the foundation for reasoning.

   > **Example Triples**:
   > * `(water, boilingPoint, 100Â°C)`
   > * `(mammals, class, vertebrates)`
   > * `(photosynthesis, requires, sunlight)`

2. **Explanation Hints (CoK-EH)**
   Explanation hints provide logical connections between evidence triples to construct valid reasoning chains. They explicitly state how the presented evidence supports the final conclusion.

### Example prompt:
```markdown
Question: Determine if a plant can grow in a windowless room.

Evidence Triples:
 - (plants, require, photosynthesis)
 - (photosynthesis, requires, sunlight)
 - (windowless room, lacks, sunlight)
 

Explanation:

Plants require photosynthesis for growth, which needs sunlight. A windowless room lacks sunlight.

Answer: No, a plant cannot grow in a windowless room due to the absence of sunlight required for photosynthesis.
```

### Prompt:
```markdown
Question: [Input question]

Evidence:
 - (subject, relation, object)
 - (subject, relation, object)
 

...

Explanation: [Logical connection between evidence]

Answer: [Conclusion]

[Your question]
```

# `10.` Chain of Draft

ğŸ”— Link: [CoD](https://learnprompting.org/docs/advanced/thought_generation/chain-of-draft)

- It reduces the tokens size of the prompt dramatically compared to CoT.
- It uses abstract thinking steps instead of full fleged thinking done in CoT.

### Prompt
```markdown
Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.
Guidelines:
 - Limit each step to 5 words
 - Focus on essential calculations/transformations
 - Maintain logical progression
 - Mark final answer with ####
```

# `11.` Contrastive Chain-of-Thought (CCoT)

ğŸ”— Link: [Contrastive CoT](https://learnprompting.org/docs/advanced/thought_generation/contrastive_cot)

> It enables the model **to spot it's mistakes** while reasoning.

* This is a few-shot prompt technique. You need to put in the exampels.
* The examples **must include the reasoning steps** positive and negative.

### Prompt (for reference)
```markdown
Q: [Demo Question 1]
Correct reasoning: [Tâº]
Correct answer: [Aâº]
Incorrect reasoning: [Tâ»]
Incorrect answer: [Aâ»]

Q: [Demo Question 2]
Correct reasoning: ...
...

Q: [Your New Question]
Reasoning:
Answer:
```